# SPDX-License-Identifier: AGPL-3.0-only
# Copyright (c) 2025 Jennifer Slotnick

"""
Test JSON Schema validation for preflight reports.

This module validates that reports generated by run_rulepack() conform to
the preflight_report_v1.schema.json schema.
"""

import json
from pathlib import Path

import jsonschema
import pytest

from fairy.core.services.validator import run_rulepack


def _get_schema_path() -> Path:
    """Get path to preflight_report_v1.schema.json relative to repo root."""
    # Assume we're in tests/ directory, so go up one level to repo root
    repo_root = Path(__file__).resolve().parent.parent
    schema_path = repo_root / "schemas" / "preflight_report_v1.schema.json"
    return schema_path


def _load_schema() -> dict:
    """Load and return the JSON schema."""
    schema_path = _get_schema_path()
    if not schema_path.exists():
        pytest.skip(f"Schema file not found: {schema_path}")
    return json.loads(schema_path.read_text())


def test_schema_file_exists():
    """Verify the schema file exists and is valid JSON."""
    schema_path = _get_schema_path()
    assert schema_path.exists(), f"Schema file not found: {schema_path}"
    schema = json.loads(schema_path.read_text())
    assert "$schema" in schema, "Schema missing $schema field"
    assert schema.get("$id") is not None, "Schema missing $id field"


def test_report_validates_against_schema(rulepack_path: Path, samples_path: Path, files_path: Path):
    """
    Test that a report generated by run_rulepack() validates against the JSON schema.

    This is the main validation test - it ensures the runtime code produces
    reports that conform to the schema contract.
    """
    schema = _load_schema()
    report = run_rulepack(
        rulepack_path=rulepack_path,
        samples_path=samples_path,
        files_path=files_path,
    )

    # Convert report to dict if needed (it should already be a dict)
    if not isinstance(report, dict):
        pytest.fail(f"Expected dict, got {type(report)}")

    # Validate against schema
    try:
        jsonschema.validate(instance=report, schema=schema)
    except jsonschema.ValidationError as e:
        error_msg = "Report does not conform to schema:\n"
        error_msg += f"  Message: {e.message}\n"
        error_msg += f"  Path: {'.'.join(str(p) for p in e.absolute_path)}\n"
        error_msg += f"  Schema path: {e.schema_path}\n"
        error_msg += f"  Instance value: {e.instance}\n"
        error_msg += f"  Schema value: {e.schema}"
        pytest.fail(error_msg)


def test_report_has_required_top_level_fields(
    rulepack_path: Path, samples_path: Path, files_path: Path
):
    """Test that report has all required top-level fields."""
    report = run_rulepack(
        rulepack_path=rulepack_path,
        samples_path=samples_path,
        files_path=files_path,
    )

    required_fields = [
        "schema_version",
        "generated_at",
        "dataset_id",
        "metadata",
        "summary",
        "results",
    ]

    for field in required_fields:
        assert field in report, f"Report missing required field: {field}"

    # Check schema_version format
    assert report["schema_version"] == "1.0.0", "schema_version must be '1.0.0'"

    # Check generated_at format (ISO-8601 UTC with Z)
    generated_at = report["generated_at"]
    assert isinstance(generated_at, str), "generated_at must be a string"
    assert generated_at.endswith("Z"), "generated_at must end with 'Z' (UTC)"
    # Basic format check: YYYY-MM-DDTHH:MM:SSZ
    assert "T" in generated_at, "generated_at must contain 'T' separator"

    # Check dataset_id format
    dataset_id = report["dataset_id"]
    assert isinstance(dataset_id, str), "dataset_id must be a string"
    assert dataset_id.startswith("sha256:"), "dataset_id must start with 'sha256:'"
    assert len(dataset_id) == 71, "dataset_id must be 'sha256:' (7) + 64 hex chars = 71"


def test_report_metadata_structure(rulepack_path: Path, samples_path: Path, files_path: Path):
    """Test that metadata.inputs and metadata.rulepack have correct structure."""
    report = run_rulepack(
        rulepack_path=rulepack_path,
        samples_path=samples_path,
        files_path=files_path,
    )

    metadata = report["metadata"]
    assert "inputs" in metadata, "metadata must have 'inputs'"
    assert "rulepack" in metadata, "metadata must have 'rulepack'"

    # Check inputs structure
    inputs = metadata["inputs"]
    assert isinstance(inputs, dict), "metadata.inputs must be a dict"
    for name, input_meta in inputs.items():
        assert isinstance(name, str), "Input name must be a string"
        assert "path" in input_meta, f"Input '{name}' missing 'path'"
        assert "sha256" in input_meta, f"Input '{name}' missing 'sha256'"
        assert "n_rows" in input_meta, f"Input '{name}' missing 'n_rows'"
        assert "n_cols" in input_meta, f"Input '{name}' missing 'n_cols'"
        assert "header" in input_meta, f"Input '{name}' missing 'header'"

    # Check rulepack structure
    rulepack = metadata["rulepack"]
    assert isinstance(rulepack, dict), "metadata.rulepack must be a dict"
    assert "path" in rulepack, "rulepack missing 'path'"
    assert "sha256" in rulepack, "rulepack missing 'sha256'"


def test_report_results_structure(rulepack_path: Path, samples_path: Path, files_path: Path):
    """Test that results array has correct structure."""
    report = run_rulepack(
        rulepack_path=rulepack_path,
        samples_path=samples_path,
        files_path=files_path,
    )

    results = report["results"]
    assert isinstance(results, list), "results must be an array"

    for result in results:
        assert "rule" in result, "Result missing 'rule'"
        assert "level" in result, "Result missing 'level'"
        assert "count" in result, "Result missing 'count'"
        assert "samples" in result, "Result missing 'samples'"

        rule_id = result["rule"]
        assert isinstance(rule_id, str), "rule must be a string"
        assert len(rule_id) > 0, "rule must be non-empty"

        level = result["level"]
        assert level in ["pass", "warn", "fail"], f"Invalid level: {level}"

        count = result["count"]
        assert isinstance(count, int), "count must be an integer"
        assert count >= 0, "count must be >= 0"

        samples = result["samples"]
        assert isinstance(samples, list), "samples must be an array"
        assert len(samples) <= 10, "samples must have at most 10 items"

        for sample in samples:
            # row is optional but if present must be >= 1
            if "row" in sample:
                assert isinstance(sample["row"], int), "row must be an integer"
                assert sample["row"] >= 1, "row must be >= 1"


def test_report_summary_structure(rulepack_path: Path, samples_path: Path, files_path: Path):
    """Test that summary has correct structure."""
    report = run_rulepack(
        rulepack_path=rulepack_path,
        samples_path=samples_path,
        files_path=files_path,
    )

    summary = report["summary"]
    assert "by_level" in summary, "summary missing 'by_level'"
    assert "by_rule" in summary, "summary missing 'by_rule'"

    by_level = summary["by_level"]
    assert isinstance(by_level, dict), "by_level must be a dict"
    for level, count in by_level.items():
        assert level in ["pass", "warn", "fail"], f"Invalid level in by_level: {level}"
        assert isinstance(count, int), f"Count for {level} must be an integer"
        assert count >= 0, f"Count for {level} must be >= 0"

    by_rule = summary["by_rule"]
    assert isinstance(by_rule, dict), "by_rule must be a dict"
    for rule_id, level in by_rule.items():
        assert isinstance(rule_id, str), "Rule ID in by_rule must be a string"
        assert level in ["pass", "warn", "fail"], f"Invalid level in by_rule: {level}"


def test_results_array_is_sorted(rulepack_path: Path, samples_path: Path, files_path: Path):
    """Test that results array is sorted deterministically."""
    report = run_rulepack(
        rulepack_path=rulepack_path,
        samples_path=samples_path,
        files_path=files_path,
    )

    results = report["results"]
    if len(results) < 2:
        pytest.skip("Need at least 2 results to test sorting")

    # Check that results are sorted by (meta.input, meta.column, rule, level)
    # Since we may not have meta fields, at minimum check rule is sorted
    rule_ids = [r["rule"] for r in results]
    assert rule_ids == sorted(rule_ids), "Results should be sorted by rule"


def test_samples_are_limited_and_sorted(rulepack_path: Path, samples_path: Path, files_path: Path):
    """Test that samples are limited to 10 and sorted."""
    report = run_rulepack(
        rulepack_path=rulepack_path,
        samples_path=samples_path,
        files_path=files_path,
    )

    results = report["results"]
    for result in results:
        samples = result["samples"]
        assert len(samples) <= 10, f"Result {result['rule']} has more than 10 samples"

        # If we have multiple samples, check they're sorted
        if len(samples) > 1:
            # Check sorting by (row, column, stringify(value))
            for i in range(len(samples) - 1):
                s1 = samples[i]
                s2 = samples[i + 1]

                # Compare row (0 if None)
                row1 = s1.get("row") if s1.get("row") is not None else 0
                row2 = s2.get("row") if s2.get("row") is not None else 0
                if row1 != row2:
                    assert row1 <= row2, f"Samples not sorted by row: {s1} vs {s2}"
                    continue

                # Compare column (empty string if None)
                col1 = s1.get("column") if s1.get("column") is not None else ""
                col2 = s2.get("column") if s2.get("column") is not None else ""
                if col1 != col2:
                    assert col1 <= col2, f"Samples not sorted by column: {s1} vs {s2}"
                    continue

                # Compare value (stringified)
                val1 = str(s1.get("value")) if s1.get("value") is not None else ""
                val2 = str(s2.get("value")) if s2.get("value") is not None else ""
                assert val1 <= val2, f"Samples not sorted by value: {s1} vs {s2}"


def test_summary_by_level_and_by_rule_are_correct(
    rulepack_path: Path, samples_path: Path, files_path: Path
):
    """Test that summary.by_level and summary.by_rule match the results."""
    report = run_rulepack(
        rulepack_path=rulepack_path,
        samples_path=samples_path,
        files_path=files_path,
    )

    results = report["results"]
    summary = report["summary"]

    # Count levels from results
    expected_by_level = {"pass": 0, "warn": 0, "fail": 0}
    for result in results:
        level = result["level"]
        expected_by_level[level] = expected_by_level.get(level, 0) + 1

    # Compare with summary
    by_level = summary["by_level"]
    assert by_level == expected_by_level, "by_level should match actual result levels"

    # Check by_rule: each rule should map to its level (or highest if multiple)
    by_rule = summary["by_rule"]
    for result in results:
        rule = result["rule"]
        level = result["level"]
        if rule in by_rule:
            # If rule appears multiple times, precedence is fail > warn > pass
            current_level = by_rule[rule]
            if level == "fail" or (level == "warn" and current_level == "pass"):
                assert (
                    by_rule[rule] == level
                ), f"by_rule[{rule}] should be {level} (highest severity)"
        else:
            # First occurrence
            assert by_rule[rule] == level, f"by_rule[{rule}] should be {level}"


def test_metadata_inputs_includes_all_inputs(
    rulepack_path: Path, samples_path: Path, files_path: Path
):
    """Test that metadata.inputs includes all inputs used."""
    report = run_rulepack(
        rulepack_path=rulepack_path,
        samples_path=samples_path,
        files_path=files_path,
    )

    metadata = report["metadata"]
    inputs = metadata["inputs"]

    # Should have at least "samples" and "files"
    assert "samples" in inputs, "metadata.inputs should include 'samples'"
    assert "files" in inputs, "metadata.inputs should include 'files'"

    # Check that input keys are sorted (deterministic ordering)
    input_names = list(inputs.keys())
    assert input_names == sorted(input_names), "Input keys should be sorted alphabetically"

    # Verify each input has required fields
    for name, input_meta in inputs.items():
        assert "path" in input_meta, f"Input '{name}' missing 'path'"
        assert "sha256" in input_meta, f"Input '{name}' missing 'sha256'"
        assert "n_rows" in input_meta, f"Input '{name}' missing 'n_rows'"
        assert "n_cols" in input_meta, f"Input '{name}' missing 'n_cols'"
        assert "header" in input_meta, f"Input '{name}' missing 'header'"


def test_metadata_rulepack_is_populated(rulepack_path: Path, samples_path: Path, files_path: Path):
    """Test that metadata.rulepack is populated with correct fields."""
    report = run_rulepack(
        rulepack_path=rulepack_path,
        samples_path=samples_path,
        files_path=files_path,
    )

    metadata = report["metadata"]
    rulepack = metadata["rulepack"]

    # Required fields
    assert "path" in rulepack, "rulepack missing 'path'"
    assert "sha256" in rulepack, "rulepack missing 'sha256'"

    # Path should match the input
    assert rulepack["path"] == str(rulepack_path.resolve()), "rulepack path should match input"

    # SHA256 should be 64 hex chars
    sha256_val = rulepack["sha256"]
    assert isinstance(sha256_val, str), "rulepack sha256 must be a string"
    assert len(sha256_val) == 64, "rulepack sha256 must be 64 hex chars"
    assert all(c in "0123456789abcdef" for c in sha256_val), "rulepack sha256 must be hex"

    # Optional fields (may or may not be present)
    if "id" in rulepack:
        assert isinstance(rulepack["id"], str), "rulepack id must be a string"
    if "version" in rulepack:
        assert isinstance(rulepack["version"], str), "rulepack version must be a string"
    if "params_sha256" in rulepack and rulepack["params_sha256"] is not None:
        assert isinstance(rulepack["params_sha256"], str), "rulepack params_sha256 must be a string"
        assert len(rulepack["params_sha256"]) == 64, "rulepack params_sha256 must be 64 hex chars"


def test_dataset_id_is_computed_correctly(
    rulepack_path: Path, samples_path: Path, files_path: Path
):
    """Test that dataset_id is computed correctly from all inputs."""
    from fairy.core.services.provenance import (
        CANON_VERSION_V1,
        compute_dataset_id,
        compute_params_sha256,
    )

    report = run_rulepack(
        rulepack_path=rulepack_path,
        samples_path=samples_path,
        files_path=files_path,
    )

    dataset_id = report["dataset_id"]
    assert dataset_id.startswith("sha256:"), "dataset_id must start with 'sha256:'"
    assert len(dataset_id) == 71, "dataset_id must be 'sha256:' (7) + 64 hex chars = 71"

    metadata = report["metadata"]
    inputs = metadata["inputs"]
    rp = metadata["rulepack"]

    # v1 identity uses only input sha256s
    inputs_sha256 = {name: meta["sha256"] for name, meta in inputs.items()}

    # params are currently passed as {} in tests/CLI; stil must be hashed deterministitcally
    params_sha256 = compute_params_sha256({})

    expected_dataset_id = compute_dataset_id(
        inputs_sha256=inputs_sha256,
        rulepack={
            "id": rp["id"],
            "version": rp["version"],
            "sha256": rp["sha256"],
        },
        params_sha256=params_sha256,
        canon_version=CANON_VERSION_V1,
    )

    assert dataset_id == expected_dataset_id, "dataset_id should match manual computation"


def test_generated_at_format(rulepack_path: Path, samples_path: Path, files_path: Path):
    """Test that generated_at is in ISO-8601 UTC format with Z suffix."""
    import re

    report = run_rulepack(
        rulepack_path=rulepack_path,
        samples_path=samples_path,
        files_path=files_path,
    )

    generated_at = report["generated_at"]
    assert isinstance(generated_at, str), "generated_at must be a string"
    assert generated_at.endswith("Z"), "generated_at must end with 'Z' (UTC)"

    # Pattern: YYYY-MM-DDTHH:MM:SS[.fraction]Z (allows optional milliseconds)
    pattern = r"^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(\.\d+)?Z$"
    assert re.match(
        pattern, generated_at
    ), f"generated_at must match pattern {pattern}, got {generated_at}"

    # Verify it's a valid timestamp (can be parsed)
    from datetime import datetime

    try:
        # Remove Z and parse as UTC
        timestamp_str = generated_at.replace("Z", "+00:00")
        datetime.fromisoformat(timestamp_str)
    except ValueError as e:
        pytest.fail(f"generated_at is not a valid ISO-8601 timestamp: {e}")
